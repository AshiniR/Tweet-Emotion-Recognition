# ğŸ“ Tweet Emotion Recognition

 DAIR AI Emotion Dataset 
# ğŸ§  Introduction
Understanding emotions expressed in text is a key challenge in Natural Language Processing (NLP). This project focuses on Tweet Emotion Recognition â€” automatically classifying tweets into six emotion categories using a variety of models ranging from traditional machine learning to advanced transformer-based architectures.

* The project experiments with multiple models:
* SVM (Support Vector Machine)
* BLSTM (Bidirectional LSTM)
* DistilBERT
* BERT
* RoBERTa

The objective is to compare their performance on emotion detection tasks and identify the most effective model for real-world applications.

# ğŸ¯Objectives

* Preprocess and clean tweet text data (remove mentions, links, hashtags, and emojis).
* Train and evaluate multiple models for emotion classification.
* Fine-tune transformer-based models (DistilBERT, BERT, RoBERTa) for multi-class emotion recognition.
* Address class imbalance using weighted loss functions and weighted metrics.
* Compare model performance using accuracy, precision, recall, and F1-score.
* Save the best-performing models for deployment.

# ğŸ“Š Dataset

Source: Tweet Emotion Recognition Dataset (Kaggle)
This dataset consists of tweets labeled with six emotions.

* 0	- Sadness	
* 1	- Joy	
* 2	- Love	
* 3	- Anger	
* 4	- Fear	
* 5	- Surprise

# ğŸ§¹ Preprocessing Steps

* Convert text to lowercase (SVM)
* Remove URLs, mentions (@username), and hashtags(SVM)
* Tokenize using the respective tokenizer (BERT / RoBERTa / DistilBERT)
* Apply dynamic padding and truncation
* Encode labels numerically
* Handle class imbalance using computed class weights

âš™ï¸ Evaluation Metrics

Each model was evaluated using the following weighted metrics to account for class imbalance:

* Accuracy
* Weighted Precision
* Weighted Recall
* Weighted F1-Score

# ğŸ“ Project Structure

```plaintext
tweet-emotion-recognition/
â”‚
â”œâ”€â”€ Data/
â”‚   â””â”€â”€              
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ tweet-emotion-recognition.ipynb  
â”‚
â”œâ”€â”€ README.md                         
â”œâ”€â”€ .gitignore
```
# Key Technologies
ğŸ¤— Transformers: Hugging Face transformers library
ğŸ”¥ PyTorch: Deep learning framework (BERT, RoBERTa, DistilBERT)
ğŸ’» TensorFlow / Keras: Deep learning framework (BLSTM)
ğŸ“Š Scikit-learn: Evaluation metrics, preprocessing, Traditional ML models (SVM) and feature extraction
ğŸ¯ Optuna: Hyperparameter optimization
ğŸ“ˆ Matplotlib/Seaborn: Data visualization
ğŸ¼ Pandas: Data manipulation

# ğŸ† Results and Comparison

| Model       | Accuracy | Precision (W) | Recall (W) | F1-Score (W) |
| ----------- | -------- | ------------- | ---------- | ------------ |
| SVM         | 0.85     | 0.84          | 0.85       | 0.84         |
| BLSTM       | 0.88     | 0.87          | 0.88       | 0.87         |
| DistilBERT  | 0.91     | 0.91          | 0.91       | 0.91         |
| BERT        | 0.93     | 0.93          | 0.93       | 0.93         |
| **RoBERTa** | **0.95** | **0.95**      | **0.95**   | **0.95**     |

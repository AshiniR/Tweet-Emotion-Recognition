# ğŸ“ Tweet Emotion Recognition

# ğŸ§  Introduction
Understanding emotions expressed in text is a key challenge in Natural Language Processing (NLP). This project focuses on Tweet Emotion Recognition â€” automatically classifying tweets into six emotion categories using a variety of models ranging from traditional machine learning to advanced transformer-based architectures.

* The project experiments with multiple models:
* SVM (Support Vector Machine)
* BLSTM (Bidirectional LSTM)
* DistilBERT
* BERT
* RoBERTa

The objective is to compare their performance on emotion detection tasks and identify the most effective model for real-world applications.

# ğŸ¯Objectives

* Preprocess and clean tweet text data (remove mentions, links, hashtags, and emojis).
* Train and evaluate multiple models for emotion classification.
* Fine-tune transformer-based models (DistilBERT, BERT, RoBERTa) for multi-class emotion recognition.
* Address class imbalance using weighted loss functions and weighted metrics.
* Compare model performance using accuracy, precision, recall, and F1-score.
* Save the best-performing models for deployment.

# ğŸ“Š Dataset

Source: Tweet Emotion Recognition Dataset (Kaggle)( DAIR AI Emotion Dataset )
This dataset consists of tweets labeled with six emotions.

* 0	- Sadness	
* 1	- Joy	
* 2	- Love	
* 3	- Anger	
* 4	- Fear	
* 5	- Surprise

# ğŸ§¹ Preprocessing Steps

* Convert text to lowercase (SVM)
* Remove URLs, mentions (@username), and hashtags(SVM)
* Tokenize using the respective tokenizer (BERT / RoBERTa / DistilBERT)
* Apply dynamic padding and truncation
* Encode labels numerically
* Handle class imbalance using computed class weights

# âš™ï¸ Evaluation Metrics

Each model was evaluated using the following weighted metrics to account for class imbalance:

* Accuracy
* Weighted Precision
* Weighted Recall
* Weighted F1-Score

# ğŸ“ Project Structure

```plaintext
tweet-emotion-recognition/
â”‚
â”œâ”€â”€ Data/emotions   
â”‚   â””â”€â”€ test.txt
|             
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ tweet-emotion-recognition.ipynb  
â”‚
â”œâ”€â”€ README.md                         
â”œâ”€â”€ .gitignore
```
# Key Technologies
* ğŸ¤— Transformers: Hugging Face transformers library
* ğŸ”¥ PyTorch: Deep learning framework (BERT, RoBERTa, DistilBERT)
* ğŸ’» TensorFlow / Keras: Deep learning framework (BLSTM)
* ğŸ“Š Scikit-learn: Evaluation metrics, preprocessing, Traditional ML models (SVM) and feature extraction
* ğŸ¯ Optuna: Hyperparameter optimization
* ğŸ“ˆ Matplotlib/Seaborn: Data visualization
* ğŸ¼ Pandas: Data manipulation

# ğŸ† Results and Comparison

| Model      | Accuracy | F1 (Weighted) | Precision (Weighted) | Recall (Weighted) |
| ---------- | -------- | ------------- | -------------------- | ----------------- |
| SVM        | 0.8915   | 0.8947        | 0.9040               | 0.8915            |
| BLSTM      | 0.9245   | 0.9219        | 0.9260               | 0.9245            |
| DistilBERT | 0.9300   | 0.9308        | 0.9328               | 0.9300            |
| BERT       | 0.9295   | 0.9309        | 0.9356               | 0.9295            |
| RoBERTa    | 0.9220   | 0.9239        | 0.9313               | 0.9220            |

Based on the evaluation metrics, DistilBERT is the best-performing model for this task. It achieves the highest overall accuracy (0.9300) and maintains strong weighted F1 (0.9308), precision (0.9328), and recall (0.9300), providing the best balance between all metrics.
